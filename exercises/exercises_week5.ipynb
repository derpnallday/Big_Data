{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Machine Learning 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "There are innumerable machine learning models (and algorithms for fitting them to data) out there, and each one has something special about it that makes it suitable to a specific type of problem. To apply machine learning and get some initial results is fairly straight forward. Getting under the hood, however, requires a bit of work. This week we will look into how Decision Trees and PCA works. In the exercises today you will:\n",
    "\n",
    "* Implement a standard decision tree mechanism\n",
    "* Play around with a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get started**: Load the data you used last week to classify hero/villainness. It should consist of two arrays, one of dimensions ($N_{characters}$, $N_{teams}$) which is your feature matrix that has the team alliances of each character as a row vector of ones and zeros, and another of dimensions ($N_{characters}$, ) which is your target array that gives whether a character is a hero (1) or a villain (0). You can either load the data or copy/paste the code that generates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import numpy as np\n",
    "\n",
    "def get_alliances(char, faction=None):\n",
    "    \"\"\"Return list of alliances for Marvel character.\"\"\"\n",
    "    \n",
    "    if faction is None:\n",
    "        for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "            faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "            if char in faction_chars:\n",
    "                break\n",
    "    \n",
    "    # Load character markup\n",
    "    with open(\"./data/%s/%s.txt\" % (faction, char)) as fp:\n",
    "        markup = fp.read()\n",
    "\n",
    "    # Get alliance field\n",
    "    alliances_field = re.findall(r\"alliances[\\w\\W]+?\\|.+=\", markup)\n",
    "    if alliances_field == []:\n",
    "        return []\n",
    "\n",
    "    # Extract teams from alliance field\n",
    "    return [t[2:-1] for t in re.findall(r\"\\[\\[.+?[\\]\\|]\", alliances_field[0][10:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622\n"
     ]
    }
   ],
   "source": [
    "all_teams=[]\n",
    "seen = set(all_teams)\n",
    "\n",
    "#add all unique teams to list\n",
    "for faction in [\"heroes\", \"ambiguous\", \"villains\"]:\n",
    "    faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "    \n",
    "    for char in faction_chars:\n",
    "        alliances = get_alliances(char)\n",
    "        \n",
    "        for team in alliances:\n",
    "            seen.add(team)\n",
    "\n",
    "for team in seen:\n",
    "    all_teams.append(team)\n",
    "\n",
    "all_teams.sort()    \n",
    "  \n",
    "print(len(all_teams))    \n",
    "#print(all_teams)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_alliance(char):\n",
    "    \"\"\"returns vector of 0,1 whether char is in team referenced to all_teams array\"\"\"\n",
    "    vector = []\n",
    "    alliances = get_alliances(char)\n",
    "    c = 0\n",
    "    \n",
    "    for team in all_teams:\n",
    "        if team not in alliances:\n",
    "            vector.append(0)\n",
    "        else:\n",
    "            vector.append(1)\n",
    "            c+=1\n",
    "    \n",
    "    \n",
    "    return vector, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create matrix\n",
    "matrix = []\n",
    "target = []\n",
    "chars = []\n",
    "for faction in [\"heroes\", \"villains\"]:\n",
    "    faction_chars = [c[:-4] for c in os.listdir(\"./data/%s\" % faction)]\n",
    "    \n",
    "    for char in faction_chars:\n",
    "        vector, length = encode_alliance(char)\n",
    "   \n",
    "        #only add characters in a team\n",
    "        if length >= 1:       \n",
    "            matrix.append(vector)\n",
    "            chars.append(char)\n",
    "        \n",
    "            #append the faction of the character to target\n",
    "            if faction == \"heroes\":\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "\n",
      "[1 1 1 ... 0 0 0]\n",
      "(1096, 622)\n",
      "(1096,)\n"
     ]
    }
   ],
   "source": [
    "#load check\n",
    "print(np.array(matrix))\n",
    "print(\"\\n\")\n",
    "print(np.array(target))\n",
    "\n",
    "print(np.shape(matrix))\n",
    "print(np.shape(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the decision making algorithm of a decision tree classifier, step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.1.1**: Read about [Shannon entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)).\n",
    "1. What is it? How is it defined mathematically (write out the formula in LateX formatting)?\n",
    "> The entropy can be written as\n",
    "> $$ \\text{shanon entropy }H(X)=- \\sum^n_{i=1}P(x_i)\\log_b P(x_i)$$\n",
    "> where $b$ is the base of the logarithm used (ie. 2, $e$, 10)\n",
    ">\n",
    "> input: probability vector (a list of values between 0 and 1 which sums to 1)\n",
    ">\n",
    "> output: entropy (the randomness of the result)\n",
    ">\n",
    "> this can be used in the construction of decision trees to score whether a question is good as it tells how it splits the data (low entropy is good)\n",
    ">\n",
    "> ex. p = [1,0]\n",
    "> $$ \\text{ Entropy } = -(1 * log_2(1) +0 * log_2(0)) = 0 $$\n",
    ">\n",
    "\n",
    "\n",
    "2. Write a function that computes the Shannon-entropy of a probability vector. Compute the Shannon entropy of `p=[0.4, 0.6]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shanon_entropy(p):\n",
    "    entropy = 0\n",
    "    \n",
    "    for i in p:\n",
    "        #if entry is 0 entropy must be 1\n",
    "        if i == 0:\n",
    "            return 1\n",
    "        \n",
    "        entropy += i * np.log2(i)\n",
    "        \n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=[0.4, 0.6]\n",
    "shanon_entropy(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.1.2**: Split your data into two subsets. One where characters are affiliated with X-men and one where they are not.\n",
    "1. What is the entropy of target labels in each subset?\n",
    "2. What is the weighted average entropy of the split?\n",
    "3. Write a function that computes the weighted average entropy of a split, given the data and team (name or id) on which to split the data.\n",
    "4. Plot the distribution of split entropy for all features. Comment on the result. My figure looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.2.2.4.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data on team\n",
    "def split_team(team):\n",
    "    #find team ind\n",
    "    ind = 0\n",
    "    for i in all_teams:\n",
    "        if i == team:\n",
    "            break\n",
    "        ind+=1\n",
    "    \n",
    "    team = []\n",
    "    team_ind = []\n",
    "    \n",
    "    non_team = []\n",
    "    non_ind = []\n",
    "    \n",
    "    x_ind=0\n",
    "    for char in matrix:\n",
    "        if char[ind] == 1:\n",
    "            team.append(char)\n",
    "            team_ind.append(x_ind)\n",
    "        else:\n",
    "            non_team.append(char)\n",
    "            non_ind.append(x_ind)\n",
    "        x_ind+=1\n",
    "    \n",
    "    return team, team_ind, non_team, non_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into X-men and non-X-Men\n",
    "X_men, X_men_ind, reg_men, reg_men_ind = split_team('X-Men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_men_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab target values\n",
    "X_men_target=[]\n",
    "reg_men_target=[]\n",
    "\n",
    "for i in X_men_ind:\n",
    "    X_men_target.append(target[i])\n",
    "\n",
    "for i in reg_men_ind:\n",
    "    reg_men_target.append(target[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_men_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find ratio of 0-1 for both sets\n",
    "def hero_ratio(target):\n",
    "    total = len(target)\n",
    "  \n",
    "    #team was emptyl entropy is 1\n",
    "    if total == 0:\n",
    "        return [1,0]\n",
    "        \n",
    "    ones = 0\n",
    "    for i in target:\n",
    "        if i == 1:\n",
    "            ones+=1\n",
    "    \n",
    "    ones_ratio = ones/total\n",
    "    zero_ratio = 1-ones_ratio\n",
    "    return [zero_ratio, ones_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "villian / hero ratio\n",
      "x-men ratios\n",
      "[0.08045977011494254, 0.9195402298850575]\n",
      "other ratios\n",
      "[0.5441030723488602, 0.45589692765113976]\n"
     ]
    }
   ],
   "source": [
    "print(\"villian / hero ratio\")\n",
    "\n",
    "print(\"x-men ratios\")\n",
    "X_ratio = hero_ratio(X_men_target)\n",
    "print(X_ratio)\n",
    "\n",
    "print(\"other ratios\")\n",
    "reg_ratio = hero_ratio(reg_men_target) \n",
    "print(reg_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40379715049939247\n",
      "0.9943803822497996\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "#entropy\n",
    "X_entropy = shanon_entropy(X_ratio)\n",
    "reg_entropy = shanon_entropy(reg_ratio)\n",
    "\n",
    "print(X_entropy)\n",
    "print(reg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 \n",
    "#weighted sum\n",
    "def wsum_entropy(a, b, a_ent, b_ent):\n",
    "    total = len(a) + len(b)\n",
    "   \n",
    "    if len(a) == 0:\n",
    "        return 1\n",
    "    \n",
    "    w_a = len(a)/total\n",
    "    w_b = len(b)/total\n",
    "    \n",
    "    return w_a * a_ent + w_b * b_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9475001439630428"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE: regular target includes characters with no team\n",
    "wsum_entropy(X_men_target, reg_men_target, X_entropy, reg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "#weighted entropy of teams\n",
    "def team_wsum_entropy(team):\n",
    "    team, team_ind, non_team, non_ind = split_team(team)\n",
    "   \n",
    "    team_target=[]\n",
    "    non_target=[]\n",
    "\n",
    "    for i in team_ind:\n",
    "        team_target.append(target[i])\n",
    "\n",
    "    for i in non_ind:\n",
    "        non_target.append(target[i])\n",
    "        \n",
    "    #ratios\n",
    "    team_ratio = hero_ratio(team_target)\n",
    "    non_ratio = hero_ratio(non_target)\n",
    "    \n",
    "    #entropy\n",
    "    team_entropy = shanon_entropy(team_ratio)\n",
    "    non_entropy = shanon_entropy(non_ratio)\n",
    "    \n",
    "    #weighted sum\n",
    "    return wsum_entropy(team_target, non_target, \n",
    "                        team_entropy, non_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9353200324332954"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team_wsum_entropy('Avengers (comics)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.1.3**: Print the maximum entropy path of a decision tree.\n",
    ">\n",
    ">1. Implement the following pseudocode and print the output:<br><br>\n",
    ">Step 1. Find `team` that gives lowest split entropy for `data`. Print `team`.<br>\n",
    ">Step 2. Split `data` on `team`, to produce `data0` and `data1`. Print the entropy of each, as well as their weighted avg. entropy.<br>\n",
    ">Step 3. Overwrite the `data` variable with either `data0` or `data1`, depending on which has the highest entropy.<br>\n",
    ">Step 4. Stop if there are less than 5 datapoints in `data`. Otherwise start over from 1.<br><br>\n",
    ">My output looks [like this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_6.2.3.1.png) for the first five splits.<br><br>\n",
    ">\n",
    ">2. Comment on decision path your code takes: How splits are there? Do you notice anything interesting about the final splits? Why do we choose to stop splitting before `data` get smaller than 5?\n",
    ">3. Train a `sklearn.tree.DecisionTreeClassifier` classifier on the dataset. Initiate the classifier with `criterion='entropy'`. What are the most important features of this classifier? How does this line up with the order of the order of splits you just printed (a comment is fine)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9353200324332954\n",
      "Avengers (comics)\n"
     ]
    }
   ],
   "source": [
    "lowest_ent = 1\n",
    "lowest_team = ''\n",
    "for team in all_teams:\n",
    "    ent = team_wsum_entropy(team)\n",
    "    if ent < lowest_ent:\n",
    "        lowest_ent = ent\n",
    "        lowest_team = team\n",
    "\n",
    "print(lowest_ent)\n",
    "print(lowest_team)\n",
    "\n",
    "#split on lowest ent team\n",
    "data0, data0_ind, data1_team, data1_ind = split_team(lowest_team)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regressions are great baseline models for comparing how well a more complicated model works.\n",
    "They are literally just linear regressions where the output is *squeezed* through a `sigmoid` function,\n",
    "so the returned value is between 0 and 1 (which can be interpreted as a probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.2.1**: Implement a logistic regression model.\n",
    "Below I have implemented a *linear* regression model which takes *two* input parameter.\n",
    "Create another function, `logistic_regression` that takes again two input\n",
    "variables and returns a value between 0 and 1. Demonstrate that it works by inputting\n",
    "the data, `x=[1, 1]`, and parameters, `w0=1`, `w1=1` and `w2=0`, below, and show that it gives 0.88.\n",
    ">\n",
    ">*Hint*: The `sigmoid` function can look like this:\n",
    ">\n",
    ">        def sigmoid(x):\n",
    ">            return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T07:10:28.534347Z",
     "start_time": "2019-09-29T07:10:28.525702Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression(x, w0, w1, w2):\n",
    "    return x[0] * w0 + x[1] * w1 + w2\n",
    "\n",
    "# example\n",
    "x = [1, 1]\n",
    "w0 = 1; w1 = 1; w2 = 0\n",
    "linear_regression(x, w0, w1, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T14:01:10.834257Z",
     "start_time": "2017-08-23T14:01:10.826472Z"
    }
   },
   "source": [
    ">**Ex. 5.2.2**: *Fit* a logistic regression! You can use the `scipy` module `scipy.optimize.curve_fit`\n",
    "to fit a model to some data (i.e. find the best parameter values `w`). Below I have implemented an\n",
    "example of how to fit some data to the linear regression above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T07:15:45.099126Z",
     "start_time": "2019-09-29T07:15:45.087872Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "\n",
    "def generate_X_linear(N=200):\n",
    "    \"\"\"A little function that creates some data.\"\"\"\n",
    "    x = np.vstack([\n",
    "        np.random.normal([-2, -2], 1, size=(int(N/2), 2)),\n",
    "        np.random.normal([2, 2], 1, size=(int(N/2), 2))\n",
    "    ])\n",
    "\n",
    "    y = np.array([0] * int(N/2) + [1] * int(N/2))\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Generate input and output data\n",
    "x, y = generate_X_linear()\n",
    "\n",
    "optimal_params, cov_params = curve_fit(linear_regression, x.T, y)\n",
    "optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use the `logistic_regression` function you wrote in the previous exercise and fit it to `x` and `y`.\n",
    "> Store the optimal parameters in a variable called `optimal_params` (like above), \n",
    "> then run the code cell below to plot the predictions. Mine looks like [this](https://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_5.5.2.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T07:36:06.484274Z",
     "start_time": "2019-09-29T07:36:06.304742Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.scatter(x[:, 0], x[:, 1], c=[logistic_regression(x_, *optimal_params) for x_ in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 (extra): Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These days, neural networks are probably the hottest item in machine learning, and for good reason. Neural networks *can* be a bit of a mouthful to understand, and since I didn't talk about them in this course much (if at all), you can solve this problem if you have sufficient interest for it. That said, it is a skill well worth investing in.\n",
    "\n",
    "We will be using the deep learning library **PyTorch** to build some neural networks with which we can play around with. *Nerdnote: Why not something more high-level such as Keras with a Tensorflow backend? Well PyTorch is easier to install. And using it, it's clearer what actually happens when you fit a neural network. Finally, for those taking the ANN course it's good to see something new.*\n",
    "\n",
    "To get torch running you need to first install it. It's should be fairly straight forward, but depending on\n",
    "your machine you may have to run different commands to install it. Check out the installation guide [here](https://pytorch.org/).\n",
    "\n",
    "Once you've installed it you should be able to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:42:50.639887Z",
     "start_time": "2019-02-21T11:42:50.342085Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-eb42ca6e4af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No errors? Great! Then let's make a small neural network that we know all to well at this point, and see if we can classify points with it. First, we generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:43:20.228851Z",
     "start_time": "2019-02-21T11:43:20.220487Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_X_nonlinear(N=200, R=5):\n",
    "\n",
    "    X_inner = torch.randn(int(N/2), 2)\n",
    "\n",
    "    X_outer = torch.tensor([\n",
    "        [R*np.cos(theta), R*np.sin(theta)]\n",
    "        for theta in np.linspace(0, 2 * np.pi, int(N/2))\n",
    "    ]) + torch.randn(int(N/2), 2)\n",
    "\n",
    "    X = torch.cat([X_inner, X_outer], dim=0)\n",
    "   \n",
    "    y = torch.cat([\n",
    "        torch.zeros(int(N/2)).reshape(-1, 1),\n",
    "        torch.ones(int(N/2)).reshape(-1, 1)\n",
    "    ])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Number of training datapoints\n",
    "N = 500\n",
    "\n",
    "# Generate the data (note that code is using torch arrays now)\n",
    "x, y = generate_X_nonlinear(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this data, we can now set up a neural network and train it. We are not going to do anything fancy here, just make a simple 2-layer feed forward neural network with 2 input neurons, 3 hidden neurons and 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:45:30.703601Z",
     "start_time": "2019-02-21T11:45:30.700648Z"
    }
   },
   "outputs": [],
   "source": [
    "# The layers and their number of neurons\n",
    "sizes = [2, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to define the model. PyTorch has an API called `Sequential` that makes this pretty easy. Try to get an idea of what the below code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:46:28.285802Z",
     "start_time": "2019-02-21T11:46:28.280709Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(sizes[0], sizes[1]),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(sizes[1], sizes[2]),\n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to define a loss function. We are just going to use the sum of squared errors, and again PyTorch has got an implementation we can pick right off the shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:47:27.807695Z",
     "start_time": "2019-02-21T11:47:27.804668Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can just train it! We pick a learning rate parameter (which is how big the steps we take during\n",
    "gradient descent are), define an *optimizer* which is a wrapper for training that abstracts away all the\n",
    "usual steps. The `epochs` are simply the number of times we train on the entire dataset. Then we are ready to\n",
    "train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:52:35.129417Z",
     "start_time": "2019-02-21T11:52:34.896992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learning_rate = 1e-1\n",
    "epochs = 100\n",
    "mini_batch_size = 100\n",
    "\n",
    "# Optimization wrapper\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train\n",
    "for t in range(epochs):\n",
    "    \n",
    "    # Randomly permute the row indices to get something like:\n",
    "    # tensor([16214, 18491, 16308,  ..., 19629, 17565, 24696])\n",
    "    permutation = torch.randperm(x.size()[0])\n",
    "    \n",
    "    # Start looping over the mini-batches! Each index `k` is\n",
    "    # `mini_batch_size` values apart.\n",
    "    for k in np.arange(0, x.size()[0], mini_batch_size):\n",
    "        \n",
    "        # Extract mini-batch data. The rest is the same\n",
    "        mini_batch_indices = permutation[k:k+mini_batch_size]\n",
    "        x_ = x[mini_batch_indices, :]\n",
    "        y_ = y[mini_batch_indices, :]\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(x_)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y_)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers (i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    # Print progress (here evaluating on all the data so we can compare)\n",
    "    if t % 10 == 0:\n",
    "        loss = loss_fn(model(x), y)\n",
    "        print(t, \"train:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the errors we are showing here are the sum of squared errors. You'd have to jump through a small hoop (which is very doable) to get the accuracy. Also, this is the error on the training data. So there's no guarantee that we don't overfit here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can visualize the predictions next to the true labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:53:44.190958Z",
     "start_time": "2019-02-21T11:53:43.877249Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "class cmap_in_range:\n",
    "    \"\"\"Create map to range of colors inside given domain.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> cmap = cmap_in_range([0, 1])\n",
    "    >>> cmap(0.1)\n",
    "    (0.30392156862745101, 0.30315267411304353, 0.98816547208125938, 1.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, cmap_domain, cmap_range=[0, 1], cmap_style='rainbow'):\n",
    "        self.cmap_domain = cmap_domain\n",
    "        self.cmap_range = cmap_range\n",
    "        self.m = interp1d(cmap_domain, cmap_range)\n",
    "        self.cmap = plt.get_cmap(cmap_style)\n",
    "        \n",
    "    def __call__(self, value):\n",
    "        if not self.cmap_domain[0] <= value <= self.cmap_domain[1]:\n",
    "            raise Exception(\"Value must be inside cmap_domain.\")\n",
    "        return self.cmap(self.m(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-21T11:53:44.726555Z",
     "start_time": "2019-02-21T11:53:44.267863Z"
    }
   },
   "outputs": [],
   "source": [
    "cmap = cmap_in_range([0, 1])\n",
    "\n",
    "y_true = y.reshape(-1).numpy()\n",
    "y_pred = model(x).data.numpy().reshape(-1)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"True\", fontsize=12)\n",
    "plt.scatter(x[:, 0], x[:, 1], color=list(map(cmap, y_true)))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Predicted\", fontsize=12)\n",
    "plt.scatter(x[:, 0], x[:, 1], color=list(map(cmap, y_pred)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cooool! Alright. Your turn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 5.3.1**: Can you fit a neural network like the above (maybe with more or less layers and different number of hidden neurons in each layer) to the marvel data to predict good vs. evil?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
